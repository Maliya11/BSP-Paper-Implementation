{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the encoded features\nencoded_features_path = '/kaggle/input/ds2-extracted-features/encoded_features_7.npy'\nfeatures_array = np.load(encoded_features_path)\n\n# Load the features and labels from your numpy array\ndata = features_array  # Adjust path if needed\nX = data[:, :-1]  # All columns except the last one are features\ny_ = data[:, -1] # Last column is the label\nnumeric_labels = []\nfor label in y_:\n    if label == 'N':\n        numeric_labels.append(0)\n    if label == 'L':\n        numeric_labels.append(1)\n    if label == 'R':\n        numeric_labels.append(2)\n    if label == 'V':\n        numeric_labels.append(3)\n    if label == 'A':\n        numeric_labels.append(4)\n        \ny = numeric_labels","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define kernels and hyperparameters\nkernels = {\n    \"Linear\": {\"kernel\": \"linear\"},\n    \"Quadratic\": {\"kernel\": \"poly\", \"degree\": 2},\n    \"Fine Gaussian\": {\"kernel\": \"rbf\", \"gamma\": 0.1},\n    \"Medium Gaussian\": {\"kernel\": \"rbf\", \"gamma\": \"scale\"},\n    \"Coarse Gaussian\": {\"kernel\": \"rbf\", \"gamma\": 0.01}\n}\n\n# Initialize a results list to store metrics for each kernel\nresults = []\n\n# Helper function to calculate sensitivity and specificity\ndef calculate_sensitivity_specificity(cm):\n    sensitivities, specificities = [], []\n    for i in range(len(cm)):\n        tp = cm[i, i]\n        fn = sum(cm[i, :]) - tp\n        fp = sum(cm[:, i]) - tp\n        tn = cm.sum() - (tp + fn + fp)\n\n        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n        sensitivities.append(sensitivity)\n        specificities.append(specificity)\n\n    avg_sensitivity = np.mean(sensitivities)\n    avg_specificity = np.mean(specificities)\n    return avg_sensitivity, avg_specificity","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split into training and testing sets (80% train, 20% test)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Loop over each kernel\nfor kernel_name, params in kernels.items():\n    # Initialize SVM model with specified kernel and parameters\n    svm_model = SVC(**params, C=1.0, decision_function_shape='ovo')\n    # Train the model\n    svm_model.fit(X_train, y_train)\n    # Predict on the test set\n    y_pred = svm_model.predict(X_test)\n    # Calculate accuracy\n    accuracy = accuracy_score(y_test, y_pred)\n    # Calculate confusion matrix\n    cm = confusion_matrix(y_test, y_pred)\n    # Calculate sensitivity and specificity\n    sensitivity, specificity = calculate_sensitivity_specificity(cm)\n    # Append results to the list\n    results.append([kernel_name, accuracy, sensitivity, specificity])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print results in a table format\nprint(\"Kernel Comparison Results for DS2:\")\nprint(\"{:<15} {:<10} {:<15} {:<15}\".format(\"Kernel\", \"Accuracy\", \"Sensitivity\", \"Specificity\"))\nfor row in results:\n    print(\"{:<15} {:.4f}     {:.4f}         {:.4f}\".format(row[0], row[1], row[2], row[3]))","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}